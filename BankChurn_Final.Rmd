---
title: "Bank Customer Churn"
output:
  distill::distill_article:
    toc: true
    toc_depth: 2
---

# 1. Background

Customer Churn, also known as customer attrition, customer turnover, or customer defection, in the loss of clients or customers.

Bank, insurance companies, streaming services companies and telcom service companies, often use customer churn analysis and customer churn rates as one of their key business metrics because the cost of retaining existing customers is far less than acquiring a new one.

This analysis focuses on the behavior of bank customers who are more likely to leave the bank (i.e. close their bank account). I want to find out the most striking behaviors of customers through Exploratory Data Analysis and later on use some of the predictive analytics techniques to determine the customers who are most likely to churn.

Last, I'll build a web-app with R Shiny for the bank to monitor their customers churn risk in a real-time setting.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, comment = "")
```

## Data Loading and Preprocessing

```{r,message=FALSE}
library(tidyverse)
library(patchwork)
library(caret)
library(vcd)
library(gridExtra)
library(knitr)
library(corrplot)
library(scales)
library(lme4)
library(DMwR)
library(InformationValue)
library(ROCR)
library(rpart)
library(randomForest)
library(xgboost)
library(xgboostExplainer)
library(MASS)
library(ggmosaic)
library(e1071)
library(ranger)
library(penalized)
library(rpart.plot)
library(ggcorrplot)
library(caTools)
library(doMC)
registerDoMC(cores=4)
bankChurn <- read_csv('Churn_Modelling.csv')
```

```{r}
glimpse(bankChurn)
```

## Data Cleaning

```{r}
bankChurn <- bankChurn %>% 
  dplyr::select(-RowNumber, -CustomerId, -Surname) %>% #remove unwanted column 
  mutate(Geography = as.factor(Geography),
         Gender = as.factor(Gender),
         HasCrCard = as.factor(HasCrCard),
         IsActiveMember = as.factor(IsActiveMember),
         Exited = as.factor(Exited),
         Tenure = as.factor(Tenure),
         NumOfProducts = as.factor(NumOfProducts))
```


```{r}
# Check NA
sapply(bankChurn, function(x) sum(is.na(x)))
```

We don't have any missing data. We're good to go. 

# 2. Exploratory Data Analysis

## Data Overview
```{r}
summary(bankChurn)
```

* CreditScore: the range of credit score is from 350 to 850

* Geography: the regional bank has customers from three countries: France, Germany and Spain

* Age: the range of customer's age is from 18 to 92

* Tenure: years that the customer has stayed with the bank

* Balance: the amount of money available for withdrawal

* NumOfProducts: number of products that the customers use in the bank

* IsActiveMember: 1 indicates is active

* EstimatedSalary: customer's self-reported annual salary

* Exited: whether the customer has churned (closed the bank account), 1 indicates churn.

## Response Variable

* Exited = 0 non-churned customer

* Exited = 1 churned customer

```{r}
ggplot(bankChurn, aes(Exited, fill = Exited)) +
  geom_bar() +
  theme(legend.position = 'none')

table(bankChurn$Exited)
round(prop.table(table(bankChurn$Exited)),3)
```

We can see most of our customers did not churn.

## Overall Distribution For All Features

First are the histograms for continuous and categorical variables.

**Continuous Variable Distribution**

```{r}
bankChurn %>%
  keep(is.numeric) %>%
  gather() %>%
  ggplot() +
  geom_histogram(mapping = aes(x=value,fill=key), color="black") +
  facet_wrap(~ key, scales = "free") +
  theme_minimal() +
  theme(legend.position = 'none')
```

* Age is a bit right-skewed

* Balance is fairly normal distributed

* Most credit scores are above 600, it is possible that high quality customers will churn



**Correlation Matrix**

```{r}
numericVarName <- names(which(sapply(bankChurn, is.numeric)))
corr <- cor(bankChurn[,numericVarName], use = 'pairwise.complete.obs')
ggcorrplot(corr, lab = TRUE)

```

I don't see any high correlation between the continuous variables (i.e. no multicollinearity). So I'll keep all this continuous variables.

**Categorical Variable Distribution**

```{r}
bankChurn %>%
  dplyr::select(-Exited) %>% 
  keep(is.factor) %>%
  gather() %>%
  group_by(key, value) %>% 
  summarize(n = n()) %>% 
  ggplot() +
  geom_bar(mapping=aes(x = value, y = n, fill=key), color="black", stat='identity') + 
  coord_flip() +
  facet_wrap(~ key, scales = "free") +
  theme_minimal() +
  theme(legend.position = 'none')
  
```

Information we can get from the plots:

* We have more male customers than females.

* Customers from France (most), Germany and France.

* Most of the customers have the bank's credit card

* We have an almost equal number of active and non-active members, not a very good sign

* Most of the customers use one or two kinds of products, with a very few use three or four products

* Almost equal number of customers in different tenure groups, except 0 and 10. 

## Continuous Variables Exploration

**Age**

```{r}
age_hist <- ggplot(bankChurn, aes(x = Age, fill = Exited)) +
  geom_histogram(binwidth = 5) +
  theme_minimal() +
  scale_x_continuous(breaks = seq(0,100,by=10), labels = comma)

age_boxplot <- ggplot(bankChurn, aes(x = Exited, y = Age, fill = Exited)) +
  geom_boxplot() + 
  theme_minimal() +
  theme(legend.position = 'none')

age_hist | age_boxplot
```

Non-churned customers have a right-skewed distribution (tend to be young). Outliers above 60 years old maybe our stable customers.

Churned customers are mostly around 40 to 50. They might need to switch to other banking service for retirement purpose or whole family issue.

We cab see very clear difference between this two groups.

**Balance**

```{r}
balance_hist <- ggplot(bankChurn, aes(x = Balance, fill = Exited)) +
  geom_histogram() +
  theme_minimal() +
  scale_x_continuous(breaks = seq(0,255000,by=30000), labels = comma) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))

balance_box <- ggplot(bankChurn, aes(x = Exited, y = Balance, fill = Exited)) +
  geom_boxplot() + 
  theme_minimal() +
  theme(legend.position = 'none')

balance_hist | balance_box
```

We can see the distribution of these two groups are quite similar. 

Surprisingly some non-churned customers have lower balance than churned customers.


**Credit Score**

```{r}
credit_hist <- ggplot(bankChurn, aes(x = CreditScore, fill = Exited)) +
  geom_histogram() +
  theme_minimal() +
  #scale_x_continuous(breaks = seq(0,255000,by=30000), labels = comma) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))

credit_box <- ggplot(bankChurn, aes(x = Exited, y = CreditScore, fill = Exited)) +
  geom_boxplot() + 
  theme_minimal() +
  theme(legend.position = 'none')

credit_hist | credit_box
```

Overall similar distribution. Some customers with extremely low credit score (on the left tail) as well as with high credit score also churned, it indicates that really low and high quality customer are easily churn than the average quality customer. 

**Estimated Salary**

```{r}
estimated_hist <- ggplot(bankChurn, aes(x = EstimatedSalary, fill = Exited)) +
  geom_histogram() +
  theme_minimal() +
  #scale_x_continuous(breaks = seq(0,255000,by=30000), labels = comma) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))

estimated_box <- ggplot(bankChurn, aes(x = Exited, y = EstimatedSalary, fill = Exited)) +
  geom_boxplot() + 
  theme_minimal() +
  theme(legend.position = 'none')

estimated_hist | estimated_box
```

Both groups have a very similar distribution. Esimated Salary might not be a very important infomation to decide if a customer will churn or not.

## Categorical Variables Exploration

```{r}
gender_graph <- bankChurn %>%
  dplyr::select(Gender, Exited) %>% 
  table(.) %>% 
  as.data.frame() %>% 
  ggplot(.) +
  ggmosaic::geom_mosaic(aes(weight = Freq, x = product(Gender), fill = Exited)) +
  ggthemes::theme_tufte() +
  scale_fill_brewer(type = "qual") +
  labs(x = 'Gender')

geography_graph <- bankChurn %>%
  dplyr::select(Geography, Exited) %>% 
  table(.) %>% 
  as.data.frame() %>% 
  ggplot(.) +
  ggmosaic::geom_mosaic(aes(weight = Freq, x = product(Geography), fill = Exited)) +
  ggthemes::theme_tufte() +
  scale_fill_brewer(type = "qual") +
  labs(x = 'Geography')

tenure_graph <- bankChurn %>%
  dplyr::select(Tenure, Exited) %>% 
  table(.) %>% 
  as.data.frame() %>% 
  ggplot(.) +
  ggmosaic::geom_mosaic(aes(weight = Freq, x = product(Tenure), fill = Exited)) +
  ggthemes::theme_tufte() +
  scale_fill_brewer(type = "qual") +
  labs(x = 'Tenure')

HasCrCard_graph <- bankChurn %>%
  dplyr::select(HasCrCard, Exited) %>% 
  table(.) %>% 
  as.data.frame() %>% 
  ggplot(.) +
  ggmosaic::geom_mosaic(aes(weight = Freq, x = product(HasCrCard), fill = Exited)) +
  ggthemes::theme_tufte() +
  scale_fill_brewer(type = "qual") +
  labs(x = 'HasCrCard')

IsActiveMember_graph <- bankChurn %>%
  dplyr::select(IsActiveMember, Exited) %>% 
  table(.) %>% 
  as.data.frame() %>% 
  ggplot(.) +
  ggmosaic::geom_mosaic(aes(weight = Freq, x = product(IsActiveMember), fill = Exited)) +
  ggthemes::theme_tufte() +
  scale_fill_brewer(type = "qual") +
  labs(x = 'IsActiveMember')

NumOfProducts_graph <- bankChurn %>%
  dplyr::select(NumOfProducts, Exited) %>% 
  table(.) %>% 
  as.data.frame() %>% 
  ggplot(.) +
  ggmosaic::geom_mosaic(aes(weight = Freq, x = product(NumOfProducts), fill = Exited)) +
  ggthemes::theme_tufte() +
  scale_fill_brewer(type = "qual") +
  labs(x = 'NumOfProducts')
  

(gender_graph | geography_graph) / (IsActiveMember_graph | HasCrCard_graph ) / (tenure_graph | NumOfProducts_graph)
```

From these mosaic plots:

* Female are more likely to churn than male

* Customers in Germany are more likely to churn than customers in France and Spain

* In-active customers are more likely to churn than active (very reasonable)

* `HasCrCard` may not be a useful feature as we cannot really tell if a customer has credit card will churn or not

* Customers in different tenure groups don't have an apparent tendency to churn or stay

* Customers who use 3 or 4 product are extremely likely to churn


**Chi-square Test**: 

One of the popular ways of doing feature selection is using chi-square test.

```{r, warning=FALSE}
chi.square <- vector()
p.value <- vector()
cateVar <- bankChurn %>% 
  dplyr::select(-Exited) %>% 
  keep(is.factor)

for (i in 1:length(cateVar)) {
 p.value[i] <- chisq.test(bankChurn$Exited, unname(unlist(cateVar[i])), correct = FALSE)[3]$p.value
 chi.square[i] <- unname(chisq.test(bankChurn$Exited, unname(unlist(cateVar[i])), correct = FALSE)[1]$statistic)
}

chi_sqaure_test <- tibble(variable = names(cateVar)) %>% 
  add_column(chi.square = chi.square) %>% 
  add_column(p.value = p.value)
knitr::kable(chi_sqaure_test)
```

The chi-square for `Tenure` and `HasCrCard` are pretty small, at the same time, their p-values are greater than 0.05, so it confirms our hypothesis that these two features will not provide useful information on the reponse (target) variable. Thus I decided to drop these two variables.

```{r}
bankChurn <- bankChurn %>% 
  dplyr::select(-Tenure, -HasCrCard)
```

# 3. Build Predictive Models

Predictive Analytics use churn prediction models that predict customer churn by assessing their propensity of risk to churn. Since these models generate a small prioritized list of potential defectors, they are effective at focusing customer retention marketing programs on the subset of the customer base who are most vulnerable to churn.

## Preprocessing

**Data Partition**

I'll split the data using a stratified sampling approach.

```{r}
set.seed(1234)
sample_set <- bankChurn %>%
  pull(.) %>% 
  sample.split(SplitRatio = .7)

bankTrain <- subset(bankChurn, sample_set == TRUE)
bankTest <- subset(bankChurn, sample_set == FALSE)
```

**Class Balancing**

Let's look at the class distribution again.

```{r}
round(prop.table(table(bankChurn$Exited)),3)
```

```{r}
round(prop.table(table(bankTrain$Exited)),3)
round(prop.table(table(bankTest$Exited)),3)
```

I'll use SMOTE function from DMwR package.

```{r}
bankTrain <- SMOTE(Exited ~ ., data.frame(bankTrain), perc.over = 100, perc.under = 200)
```

```{r}
round(prop.table(table(dplyr::select(bankTrain, Exited), exclude = NULL)),4)
```

## Logistic Regression

```{r}
## Train the model
logit.mod <- glm(Exited ~., family = binomial(link = 'logit'), data = bankTrain)

## Look at the result
summary(logit.mod)

## Predict the outcomes against our test data
logit.pred.prob <- predict(logit.mod, bankTest, type = 'response')
logit.pred <- as.factor(ifelse(logit.pred.prob > 0.5, 1, 0))
```

```{r}
head(bankTest,10)
```

```{r}
head(logit.pred.prob,10)
```


View the confusion matrix of logistic regression.

```{r}
caret::confusionMatrix(logit.pred, bankTest$Exited, positive = "1")
```

## Decision Tree

Decision Tree is a non-parametric supervised learning method used for classification and regression. 

Decision tree will splits the data into multiple sets and each set is further split into subsets to arrive at a tree like structure and make a decision. Homogeneity is the basic concept that helps to determine the attribute on which a split should be made. 

A split that results into the most homogenous subset is often considered better and step by step each attribute is choosen that maximizes the homogeneity of each subset. Further, this homogeneity is measured using different ways such as Gini Index, Entropy and Information Gain.

```{r}
ctrl <-
  trainControl(method = "cv", #cross-validation
               number = 10, #10-fold
               selectionFunction = "best")

grid <- 
  expand.grid(
    .cp = seq(from=0.0001, to=0.005, by=0.0001)
  )
```

```{r}
set.seed(1234)
tree.mod <-
  train(
    Exited ~.,
    data = bankTrain,
    method = "rpart",
    metric = "Kappa",
    trControl = ctrl,
    tuneGrid = grid
  )

tree.mod
```

```{r}
## Make predictions based on our candidate model
tree.pred.prob <- predict(tree.mod, bankTest, type = "prob")
tree.pred <- predict(tree.mod, bankTest, type = "raw")
```

View the confusion Matrix of decision tree.

```{r}
caret::confusionMatrix(tree.pred, bankTest$Exited, positive = "1")
```

## Random Forest

Random Forest is often known as an ensemble of a large number of Decision Trees, that uses bootstrapped aggregation technique to choose random samples from a dataset to train each tree in the forest. The final prediction in a Random Forest is an aggregation of prediction of individual trees. 

One of the advantages of Random Forest is that, it gives out-of-bag(OOB) error estimates, which is the mean prediction error on a training sample, using the trees that do not have that training sample in their bootstrap sample. 

```{r}
## Create a control object.
ctrl <- trainControl(method = "cv",
                     number = 10,
                     selectionFunction = "best")

## Create a grid search based on the available parameters.
grid <- expand.grid(.mtry = c(1:8))

## Build the random forest model
rf.mod <- 
  train(Exited ~.,
        data = bankTrain,
        method = 'rf',
        metric = 'Kappa',
        trControl = ctrl,
        tuneGrid = grid)

rf.mod
```

```{r}
## Make the predictions
rf.pred <- predict(rf.mod, bankTest, type = "raw")
rf.pred.prob <- predict(rf.mod, bankTest, type = "prob")
```

View the confusion matrix of random forest.

```{r}
caret::confusionMatrix(rf.pred, bankTest$Exited, positive = "1")
```

## eXtreme Gradient Boosting

```{r}
## Create a control object
ctrl <-
  trainControl(method = "cv",
               number = 10,
               selectionFunction = "best")

modelLookup("xgbTree")

## Grid Search
grid <- expand.grid(
  nrounds = 40,
  max_depth = c(4,5,6,7,8),
  eta =  c(0.1,0.2,0.3,0.4,0.5),
  gamma = 0.01,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = c(0.5, 1)
)

## Build XGBoost
set.seed(1234)
xgb.mod <-
  train(
    Exited ~ .,
    data = bankTrain,
    method = "xgbTree",
    metric = "Kappa",
    trControl = ctrl,
    tuneGrid = grid
  )
?train
xgb.mod
```

```{r}
## Make the prediction
xgb.pred <- predict(xgb.mod, bankTest, type = "raw")
xgb.pred.prob <- predict(xgb.mod, bankTest, type = "prob")
```

View the confusion matrix of XGBoost.

```{r}
caret::confusionMatrix(xgb.pred, bankTest$Exited, positive = "1")
```

# 4. Compare Models' Performance

## Metrics

```{r}
## Logistic Regression
test <- bankTest$Exited
pred <- logit.pred
prob <- logit.pred.prob

# Logistic Regression ROC curve
roc.pred <- prediction(predictions = prob, labels = test)
roc.perf <- performance(roc.pred, measure = "tpr", x.measure = "fpr")
plot(roc.perf, main = "ROC Curve for Bank Churn Prediction Approaches", col = 2, lwd = 2)
abline(a = 0, b = 1, lwd = 3, lty = 2, col = 1)

## Logistic Regression Performance Metrics
accuracy <- mean(test == pred)
precision <- posPredValue(pred, test, positive = "1")
recall <- caret::sensitivity(pred, test, positive = "1")
fmeasure <- (2 * precision * recall)/(precision + recall)
confmat <- caret::confusionMatrix(pred, test, positive = "1")
kappa <- as.numeric(confmat$overall["Kappa"])
auc <- as.numeric(performance(roc.pred, measure = "auc")@y.values)
comparisons <- tibble(approach="Logistic Regression", accuracy = accuracy, fmeasure = fmeasure,kappa = kappa, auc = auc)

## Classification Tree
test <- bankTest$Exited
pred <- tree.pred
prob <- tree.pred.prob[,2]

## Classification Tree ROC Curve
roc.pred <- prediction(predictions = prob, labels = test)
roc.perf <- performance(roc.pred, measure = "tpr", x.measure = "fpr")
plot(roc.perf, col=3, lwd = 2, add=TRUE)

## Classification Tree Performance Metrics
accuracy <- mean(test == pred)
precision <- posPredValue(pred, test, positive = "1")
recall <- caret::sensitivity(pred, test, positive = "1")
fmeasure <- (2 * precision * recall)/(precision + recall)
confmat <- caret::confusionMatrix(pred, test, positive = "1")
kappa <- as.numeric(confmat$overall["Kappa"])
auc <- as.numeric(performance(roc.pred, measure = "auc")@y.values)
comparisons <- comparisons %>%
  add_row(approach="Classification Tree", accuracy = accuracy, fmeasure = fmeasure, kappa = kappa, auc = auc) 

## Random Forest
test <- bankTest$Exited
pred <- rf.pred
prob <- rf.pred.prob[,2]

## Random Forest ROC Curve
roc.pred <- prediction(predictions = prob, labels = test)
roc.perf <- performance(roc.pred, measure = "tpr", x.measure = "fpr")
plot(roc.perf, col=4, lwd = 2, add=TRUE)

## Random Forest Performance Metrics
accuracy <- mean(test == pred)
precision <- posPredValue(pred, test, positive = "1")
recall <- caret::sensitivity(pred, test, positive = "1")
fmeasure <- (2 * precision * recall)/(precision + recall)
confmat <- caret::confusionMatrix(pred, test, positive = "1")
kappa <- as.numeric(confmat$overall["Kappa"])
auc <- as.numeric(performance(roc.pred, measure = "auc")@y.values)
comparisons <- comparisons %>%
  add_row(approach="Random Forest", accuracy = accuracy, fmeasure = fmeasure, kappa = kappa, auc = auc) 

## XGBoost
test <- bankTest$Exited
pred <- xgb.pred
prob <- xgb.pred.prob[,2]

# Plot ROC Curve.
roc.pred <- prediction(predictions = prob, labels = test)
roc.perf <- performance(roc.pred, measure = "tpr", x.measure = "fpr")
plot(roc.perf, col=5, lwd = 2, add=TRUE)

# Get performance metrics.
accuracy <- mean(test == pred)
precision <- posPredValue(pred, test, positive = "1")
recall <- caret::sensitivity(pred, test, positive = "1")
fmeasure <- (2 * precision * recall)/(precision + recall)
confmat <- caret::confusionMatrix(pred, test, positive = "1")
kappa <- as.numeric(confmat$overall["Kappa"])
auc <- as.numeric(performance(roc.pred, measure = "auc")@y.values)
comparisons <- comparisons %>%
  add_row(approach="eXtreme Gradient Boosting", accuracy = accuracy, fmeasure = fmeasure, kappa = kappa, auc = auc) 

# Draw ROC legend.
legend(0.6, 0.6, c('Logistic Regression', 'Classification Tree', 'Random Forest', 'eXtreme Gradient Boosting'), 2:5)

```

Output the comparison table.

```{r}
knitr::kable(comparisons)
```

We already know that the response variable is quite imbalanced, so I will not use prediction accuracy as our only metric, instead, I will use several metrics here to select the best model.

From the ROC curve and the comparison table,


As we can see from the ROC curve graph and the comparison table, **XGBoost** achieves a better performance. I'll go with XGBoost as our final model.

## Feature Importance

First is the feature importance plot of Random Forest model. It shows the most significant attribute in decreasing order by mean decrease in Gini. The Mean decrease Gini measures how pure the nodes are at the end of the tree. Higher the Gini Index, better is the homogeneity.

```{r}
vip::vip(rf.mod) 
```


Then we can have a look at the feature importance of XGBoost model.

```{r}
vip::vip(xgb.mod) 

```

Based on the two feature importance graphs above, we can see the important features are very similar based on the two approaches.

Age has the highest impact on customer churn, that is probably because when people get older, they need to take care of the entire family, so they switched to another bank. The bank can come up with some preferential policies to engage with those people. 

Also, we can see the number of products used by the customer has a very huge impact on customer churn. As we have discussed in the exploratory analysis, people who use more than two products are very likely to churn. So the bank should start examing why this could happen. Maybe the bank's rewarding programs have some sorts of limitation, or maybe their loan interest is not very attractive? 

Another interesting thing we can see here is that customers from Germany are more likely to churn compared to customer from France and Spain, so the marketing team in Germany should spend more time thinking of retaining Germany customers.

# 5. Shiny App

```{r, echo = FALSE}
knitr::include_graphics('shinyimage.png')
```

Shiny is an R package that makes it easy to build interactive web apps straight from R. Based on the model I trained above, it is very important for the bank to deploy the model and identify risky customers in the future.

Therefore, I made a shiny app for the bank to monitor its customer churn risk in real-time and thus increasing customer retention and engagement. 

The bank can entersome of the customer's demographic data into the web-app and then the bank can know the probability that the customer churn.

Below is a screenshot of the application. You can enter and select the information on the left side bar, and the result will show on the right.

```{r, echo = FALSE, fig.height = 15, fig.width = 15}
knitr::include_graphics('WechatIMG758.jpeg')
```

Here is a link to the <a href="https://stoneshi.shinyapps.io/bank-churn/">web-app</a>. You can also click here to get the code for <a href="https://github.com/zshi74/Bank-Churn-Prediction/tree/master/Shiny">Shiny</a>.

Happy Shiny!