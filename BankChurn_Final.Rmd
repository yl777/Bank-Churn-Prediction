---
title: "Bank Customer Churn"
output:
  distill::distill_article:
    toc: true
    toc_depth: 2
---

# 1. Data Loading and Preprocessing

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, comment = "")
```

```{r,message=FALSE}
library(tidyverse)
library(patchwork)
library(caret)
library(vcd)
library(gridExtra)
library(knitr)
library(corrplot)
library(scales)
library(lme4)
library(DMwR)
library(InformationValue)
library(ROCR)
library(rpart)
library(randomForest)
library(xgboost)
library(MASS)
library(ggmosaic)
library(e1071)
library(ranger)
library(penalized)
library(ggcorrplot)
library(caTools)
library(doMC)
registerDoMC(cores=4)
bankChurn <- read_csv('Churn_Modelling.csv')
```

```{r}
glimpse(bankChurn)
```

## Data Cleaning

```{r}
bankChurn <- bankChurn %>% 
  dplyr::select(-RowNumber, -CustomerId, -Surname) %>% #remove unwanted column 
  mutate(Geography = as.factor(Geography),
         Gender = as.factor(Gender),
         HasCrCard = as.factor(HasCrCard),
         IsActiveMember = as.factor(IsActiveMember),
         Exited = as.factor(Exited),
         Tenure = as.factor(Tenure),
         NumOfProducts = as.factor(NumOfProducts))
```


```{r}
# Check NA
sapply(bankChurn, function(x) sum(is.na(x)))
```

We don't have any missing data. We're good to go. 

# 2. Exploratory Data Analysis

## Data Overview
```{r}
summary(bankChurn)
```

* CreditScore: the range of credit score is from 350 to 850

* Geography: the regional bank has customers from three countries: France, Germany and Spain

* Age: the range of customer's age is from 18 to 92

* Tenure: years that the customer has stayed with the bank

* Balance: the amount of money available for withdrawal

* NumOfProducts: number of products that the customers use in the bank

* IsActiveMember: 1 indicates is active

* EstimatedSalary: customer's self-reported annual salary

* Exited: whether the customer has churned (closed the bank account), 1 indicates churn.

## Response Variable

* Exited = 0 non-churned customer

* Exited = 1 churned customer

```{r}
ggplot(bankChurn, aes(Exited, fill = Exited)) +
  geom_bar() +
  theme(legend.position = 'none')

table(bankChurn$Exited)
round(prop.table(table(bankChurn$Exited)),3)
```

We can see most of our customers did not churn.

## Overall Distribution For All Features

First are the histograms for continuous and categorical variables.

**Continuous Variable Distribution**

```{r}
bankChurn %>%
  keep(is.numeric) %>%
  gather() %>%
  ggplot() +
  geom_histogram(mapping = aes(x=value,fill=key), color="black") +
  facet_wrap(~ key, scales = "free") +
  theme_minimal() +
  theme(legend.position = 'none')
```

* Age is a bit right-skewed

* Balance is fairly normal distributed

* Most credit scores are above 600, it is possible that high quality customers will churn



**Correlation Matrix**

```{r}
numericVarName <- names(which(sapply(bankChurn, is.numeric)))
corr <- cor(bankChurn[,numericVarName], use = 'pairwise.complete.obs')
ggcorrplot(corr, lab = TRUE)

```

I don't see any high correlation between the continuous variables (i.e. no multicollinearity). So I'll keep all this continuous variables.

**Categorical Variable Distribution**

```{r}
bankChurn %>%
  dplyr::select(-Exited) %>% 
  keep(is.factor) %>%
  gather() %>%
  group_by(key, value) %>% 
  summarize(n = n()) %>% 
  ggplot() +
  geom_bar(mapping=aes(x = value, y = n, fill=key), color="black", stat='identity') + 
  coord_flip() +
  facet_wrap(~ key, scales = "free") +
  theme_minimal() +
  theme(legend.position = 'none')
  
```

Information we can get from the plots:

* We have more male customers than females.

* Customers from France (most), Germany and France.

* Most of the customers have the bank's credit card

* We have an almost equal number of active and non-active members, not a very good sign

* Most of the customers use one or two kinds of products, with a very few use three or four products

* Almost equal number of customers in different tenure groups, except 0 and 10. 

## Continuous Variables Exploration

**Age**

```{r}
age_hist <- ggplot(bankChurn, aes(x = Age, fill = Exited)) +
  geom_histogram(binwidth = 5) +
  theme_minimal() +
  scale_x_continuous(breaks = seq(0,100,by=10), labels = comma)

age_boxplot <- ggplot(bankChurn, aes(x = Exited, y = Age, fill = Exited)) +
  geom_boxplot() + 
  theme_minimal() +
  theme(legend.position = 'none')

age_hist | age_boxplot
```

Non-churned customers have a right-skewed distribution (tend to be young). Outliers above 60 years old maybe our stable customers.

Churned customers are mostly around 40 to 50. They might need to switch to other banking service for retirement purpose or whole family issue.

We cab see very clear difference between this two groups.

**Balance**

```{r}
balance_hist <- ggplot(bankChurn, aes(x = Balance, fill = Exited)) +
  geom_histogram() +
  theme_minimal() +
  scale_x_continuous(breaks = seq(0,255000,by=30000), labels = comma) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))

balance_box <- ggplot(bankChurn, aes(x = Exited, y = Balance, fill = Exited)) +
  geom_boxplot() + 
  theme_minimal() +
  theme(legend.position = 'none')

balance_hist | balance_box
```

We can see the distribution of these two groups are quite similar. 

Surprisingly some non-churned customers have lower balance than churned customers.


**Credit Score**

```{r}
credit_hist <- ggplot(bankChurn, aes(x = CreditScore, fill = Exited)) +
  geom_histogram() +
  theme_minimal() +
  #scale_x_continuous(breaks = seq(0,255000,by=30000), labels = comma) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))

credit_box <- ggplot(bankChurn, aes(x = Exited, y = CreditScore, fill = Exited)) +
  geom_boxplot() + 
  theme_minimal() +
  theme(legend.position = 'none')

credit_hist | credit_box
```

Overall similar distribution. Some customers with extremely low credit score (on the left tail) as well as with high credit score also churned, it indicates that really low and high quality customer are easily churn than the average quality customer. 

**Estimated Salary**

```{r}
estimated_hist <- ggplot(bankChurn, aes(x = EstimatedSalary, fill = Exited)) +
  geom_histogram() +
  theme_minimal() +
  #scale_x_continuous(breaks = seq(0,255000,by=30000), labels = comma) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))

estimated_box <- ggplot(bankChurn, aes(x = Exited, y = EstimatedSalary, fill = Exited)) +
  geom_boxplot() + 
  theme_minimal() +
  theme(legend.position = 'none')

estimated_hist | estimated_box
```

Both groups have a very similar distribution. Esimated Salary might not be a very important infomation to decide if a customer will churn or not.

## Categorical Variables Exploration

```{r}
gender_graph <- bankChurn %>%
  dplyr::select(Gender, Exited) %>% 
  table(.) %>% 
  as.data.frame() %>% 
  ggplot(.) +
  ggmosaic::geom_mosaic(aes(weight = Freq, x = product(Gender), fill = Exited)) +
  ggthemes::theme_tufte() +
  scale_fill_brewer(type = "qual") +
  labs(x = 'Gender')

geography_graph <- bankChurn %>%
  dplyr::select(Geography, Exited) %>% 
  table(.) %>% 
  as.data.frame() %>% 
  ggplot(.) +
  ggmosaic::geom_mosaic(aes(weight = Freq, x = product(Geography), fill = Exited)) +
  ggthemes::theme_tufte() +
  scale_fill_brewer(type = "qual") +
  labs(x = 'Geography')

tenure_graph <- bankChurn %>%
  dplyr::select(Tenure, Exited) %>% 
  table(.) %>% 
  as.data.frame() %>% 
  ggplot(.) +
  ggmosaic::geom_mosaic(aes(weight = Freq, x = product(Tenure), fill = Exited)) +
  ggthemes::theme_tufte() +
  scale_fill_brewer(type = "qual") +
  labs(x = 'Tenure')

HasCrCard_graph <- bankChurn %>%
  dplyr::select(HasCrCard, Exited) %>% 
  table(.) %>% 
  as.data.frame() %>% 
  ggplot(.) +
  ggmosaic::geom_mosaic(aes(weight = Freq, x = product(HasCrCard), fill = Exited)) +
  ggthemes::theme_tufte() +
  scale_fill_brewer(type = "qual") +
  labs(x = 'HasCrCard')

IsActiveMember_graph <- bankChurn %>%
  dplyr::select(IsActiveMember, Exited) %>% 
  table(.) %>% 
  as.data.frame() %>% 
  ggplot(.) +
  ggmosaic::geom_mosaic(aes(weight = Freq, x = product(IsActiveMember), fill = Exited)) +
  ggthemes::theme_tufte() +
  scale_fill_brewer(type = "qual") +
  labs(x = 'IsActiveMember')

NumOfProducts_graph <- bankChurn %>%
  dplyr::select(NumOfProducts, Exited) %>% 
  table(.) %>% 
  as.data.frame() %>% 
  ggplot(.) +
  ggmosaic::geom_mosaic(aes(weight = Freq, x = product(NumOfProducts), fill = Exited)) +
  ggthemes::theme_tufte() +
  scale_fill_brewer(type = "qual") +
  labs(x = 'NumOfProducts')
  

(gender_graph | geography_graph) / (IsActiveMember_graph | HasCrCard_graph ) / (tenure_graph | NumOfProducts_graph)
```

From these mosaic plots:

* Female are more likely to churn than male

* Customers in Germany are more likely to churn than customers in France and Spain

* In-active customers are more likely to churn than active (very reasonable)

* `HasCrCard` may not be a useful feature as we cannot really tell if a customer has credit card will churn or not

* Customers in different tenure groups don't have an apparent tendency to churn or stay

* Customers who use 3 or 4 product are extremely likely to churn


**Chi-square Test**: 

One of the popular ways of doing feature selection is using chi-square test.

```{r, warning=FALSE}
chi.square <- vector()
p.value <- vector()
cateVar <- bankChurn %>% 
  dplyr::select(-Exited) %>% 
  keep(is.factor)

for (i in 1:length(cateVar)) {
 p.value[i] <- chisq.test(bankChurn$Exited, unname(unlist(cateVar[i])), correct = FALSE)[3]$p.value
 chi.square[i] <- unname(chisq.test(bankChurn$Exited, unname(unlist(cateVar[i])), correct = FALSE)[1]$statistic)
}

chi_sqaure_test <- tibble(variable = names(cateVar)) %>% 
  add_column(chi.square = chi.square) %>% 
  add_column(p.value = p.value)
knitr::kable(chi_sqaure_test)
```

The chi-square for `Tenure` and `HasCrCard` are pretty small, at the same time, their p-values are greater than 0.05, so it confirms our hypothesis that these two features will not provide useful information on the reponse (target) variable. Thus I decided to drop these two variables.

```{r}
bankChurn <- bankChurn %>% 
  dplyr::select(-Tenure, -HasCrCard)
```

# 3. Build Predictive Models

## Preprocessing

**Data Partition**

I'll split the data using a stratified sampling approach.

```{r}
set.seed(1234)
sample_set <- bankChurn %>%
  pull(.) %>% 
  sample.split(SplitRatio = .7)

bankTrain <- subset(bankChurn, sample_set == TRUE)
bankTest <- subset(bankChurn, sample_set == FALSE)
```

**Class Balancing**

Let's look at the class distribution again.

```{r}
round(prop.table(table(bankChurn$Exited)),3)
```

```{r}
round(prop.table(table(bankTrain$Exited)),3)
round(prop.table(table(bankTest$Exited)),3)
```

I'll use SMOTE function from DMwR package.

```{r}
bankTrain <- SMOTE(Exited ~ ., data.frame(bankTrain), perc.over = 100, perc.under = 200)
```

```{r}
round(prop.table(table(dplyr::select(bankTrain, Exited), exclude = NULL)),4)
```

## Logistic Regression

```{r}
## Train the model
logit.mod <- glm(Exited ~., family = binomial(link = 'logit'), data = bankTrain)

## Look at the result
summary(logit.mod)

## Predict the outcomes against our test data
logit.pred.prob <- predict(logit.mod, bankTest, type = 'response')
logit.pred <- as.factor(ifelse(logit.pred.prob > 0.5, 1, 0))
```

```{r}
head(bankTest,10)
```

```{r}
head(logit.pred.prob,10)
```


View the confusion matrix of logistic regression.

```{r}
caret::confusionMatrix(logit.pred, bankTest$Exited, positive = "1")
```

## Decision Tree

```{r}
ctrl <-
  trainControl(method = "cv", #cross-validation
               number = 10, #10-fold
               selectionFunction = "best")

grid <- 
  expand.grid(
    .cp = seq(from=0.0001, to=0.005, by=0.0001)
  )
```

```{r}
set.seed(1234)
tree.mod <-
  train(
    Exited ~.,
    data = bankTrain,
    method = "rpart",
    metric = "Accuracy",
    trControl = ctrl,
    tuneGrid = grid
  )

tree.mod
```

```{r}
## Make predictions based on our candidate model
tree.pred.prob <- predict(tree.mod, bankTest, type = "prob")
tree.pred <- predict(tree.mod, bankTest, type = "raw")
```

View the confusion Matrix of decision tree.

```{r}
caret::confusionMatrix(tree.pred, bankTest$Exited, positive = "1")
```

## Random Forest

```{r}
## Create a control object.
ctrl <- trainControl(method = "cv",
                     number = 10,
                     selectionFunction = "best")

## Create a grid search based on the available parameters.
grid <- expand.grid(.mtry = c(1,2,3,4,5,6,7,8))

## Build the random forest model
rf.mod <- 
  train(Exited ~.,
        data = bankTrain,
        method = 'rf',
        metric = 'Accuracy',
        trControl = ctrl,
        tuneGrid = grid)

rf.mod
```

```{r}
## Make the predictions
rf.pred <- predict(rf.mod, bankTest, type = "raw")
rf.pred.prob <- predict(rf.mod, bankTest, type = "prob")
```

View the confusion matrix of random forest.

```{r}
caret::confusionMatrix(rf.pred, bankTest$Exited, positive = "1")
```

## Extreme Gradient Boosting

```{r}
## Create a control object
ctrl <-
  trainControl(method = "cv",
               number = 10,
               selectionFunction = "best")

modelLookup("xgbTree")

## Grid Search
grid <- expand.grid(
  nrounds = 40,
  max_depth = c(4,5,6,7,8),
  eta =  c(0.1,0.2,0.3,0.4,0.5),
  gamma = 0.01,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = c(0.5, 1)
)

## Build XGBoost
set.seed(1234)
xgb.mod <-
  train(
    Exited ~ .,
    data = bankTrain,
    method = "xgbTree",
    metric = "Accuracy",
    trControl = ctrl,
    tuneGrid = grid
  )

xgb.mod
```

```{r}
## Make the prediction
xgb.pred <- predict(xgb.mod, bankTest, type = "raw")
xgb.pred.prob <- predict(xgb.mod, bankTest, type = "prob")
```

View the confusion matrix of XGBoost.

```{r}
caret::confusionMatrix(xgb.pred, bankTest$Exited, positive = "1")
```

# 4. Compare Models' Performance

```{r}
## Logistic Regression
test <- bankTest$Exited
pred <- logit.pred
prob <- logit.pred.prob

# Logistic Regression ROC curve
roc.pred <- prediction(predictions = prob, labels = test)
roc.perf <- performance(roc.pred, measure = "tpr", x.measure = "fpr")
plot(roc.perf, main = "ROC Curve for Bank Churn Prediction Approaches", col = 2, lwd = 2)
abline(a = 0, b = 1, lwd = 3, lty = 2, col = 1)

## Logistic Regression Performance Metrics
accuracy <- mean(test == pred)
precision <- posPredValue(pred, test, positive = "1")
recall <- caret::sensitivity(pred, test, positive = "1")
fmeasure <- (2 * precision * recall)/(precision + recall)
confmat <- caret::confusionMatrix(pred, test, positive = "1")
kappa <- as.numeric(confmat$overall["Kappa"])
auc <- as.numeric(performance(roc.pred, measure = "auc")@y.values)
comparisons <- tibble(approach="Logistic Regression", accuracy = accuracy, fmeasure = fmeasure,kappa = kappa, auc = auc)

## Classification Tree
test <- bankTest$Exited
pred <- tree.pred
prob <- tree.pred.prob[,2]

## Classification Tree ROC Curve
roc.pred <- prediction(predictions = prob, labels = test)
roc.perf <- performance(roc.pred, measure = "tpr", x.measure = "fpr")
plot(roc.perf, col=3, lwd = 2, add=TRUE)

## Classification Tree Performance Metrics
accuracy <- mean(test == pred)
precision <- posPredValue(pred, test, positive = "1")
recall <- caret::sensitivity(pred, test, positive = "1")
fmeasure <- (2 * precision * recall)/(precision + recall)
confmat <- caret::confusionMatrix(pred, test, positive = "1")
kappa <- as.numeric(confmat$overall["Kappa"])
auc <- as.numeric(performance(roc.pred, measure = "auc")@y.values)
comparisons <- comparisons %>%
  add_row(approach="Classification Tree", accuracy = accuracy, fmeasure = fmeasure, kappa = kappa, auc = auc) 

## Random Forest
test <- bankTest$Exited
pred <- rf.pred
prob <- rf.pred.prob[,2]

## Random Forest ROC Curve
roc.pred <- prediction(predictions = prob, labels = test)
roc.perf <- performance(roc.pred, measure = "tpr", x.measure = "fpr")
plot(roc.perf, col=4, lwd = 2, add=TRUE)

## Random Forest Performance Metrics
accuracy <- mean(test == pred)
precision <- posPredValue(pred, test, positive = "1")
recall <- caret::sensitivity(pred, test, positive = "1")
fmeasure <- (2 * precision * recall)/(precision + recall)
confmat <- caret::confusionMatrix(pred, test, positive = "1")
kappa <- as.numeric(confmat$overall["Kappa"])
auc <- as.numeric(performance(roc.pred, measure = "auc")@y.values)
comparisons <- comparisons %>%
  add_row(approach="Random Forest", accuracy = accuracy, fmeasure = fmeasure, kappa = kappa, auc = auc) 

## XGBoost
test <- bankTest$Exited
pred <- xgb.pred
prob <- xgb.pred.prob[,2]

# Plot ROC Curve.
roc.pred <- prediction(predictions = prob, labels = test)
roc.perf <- performance(roc.pred, measure = "tpr", x.measure = "fpr")
plot(roc.perf, col=5, lwd = 2, add=TRUE)

# Get performance metrics.
accuracy <- mean(test == pred)
precision <- posPredValue(pred, test, positive = "1")
recall <- caret::sensitivity(pred, test, positive = "1")
fmeasure <- (2 * precision * recall)/(precision + recall)
confmat <- caret::confusionMatrix(pred, test, positive = "1")
kappa <- as.numeric(confmat$overall["Kappa"])
auc <- as.numeric(performance(roc.pred, measure = "auc")@y.values)
comparisons <- comparisons %>%
  add_row(approach="Extreme Gradient Boosting", accuracy = accuracy, fmeasure = fmeasure, kappa = kappa, auc = auc) 

# Draw ROC legend.
legend(0.6, 0.6, c('Logistic Regression', 'Classification Tree', 'Random Forest', 'eXtreme Gradient Boosting'), 2:5)

```

Output the comparison table.

```{r}
comparisons
```

As we can see from the ROC curve graph and the comparison table, **XGBoost** achieves a better performance in terms of the *AUC* value. I'll go with XGBoost as our final model.

# 5. Shiny App

I've made a shiny web-app for the bank as well. The bank can enter  some of the customer's demographic data into the web-app and then the bank can know the probability that the customer churn.

Here is a link to the <a href="https://stoneshi.shinyapps.io/BankChurn/">web-app</a>.

Enjoy using the app :).
